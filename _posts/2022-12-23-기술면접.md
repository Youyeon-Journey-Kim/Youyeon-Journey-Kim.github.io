---
layout: post
title: 기술면접 준비
tags: [Data analyst,기술면접]
math: true
date: 2022-11-16 12:49 +0800
---


#### 기술면접/실무면접을 위해 알고있던 개념이라도 확실히 짚고 넘어가고 싶어서 정리를 하기로 했다. 

| 분야 | 용어 | 용어 풀이 |
| --- | --- | --- |
| 머신러닝 일반 | 머신러닝 | ​Machine learning(기계학습) : (톰 미첼의 정의에 따르면)컴퓨터가 어떤 작업을 하는데 있어 경험으로 부터 학습하여 성능을 향상 시키는 것이다. |
| 머신러닝 일반 | ​딥러닝 | Deep learning :머신러닝의 한 분야로, 심층 신경망을 이용하여 다량의 데이터에서 표현되는 핵심을 찾아내어 일반화하고, 이것을 새로운 데이터에 대해 예측하는 데 사용하는 기술이다.  |
| 머신러닝 일반 | ​머신러닝의 종류 | **지도학습(Supervised learning)** : 데이터에 대한 정답을 주고 학습시키는 방법으로, 분류나 회귀에 주로 쓰인다. **비지도학습(Unsupervised learning)**: 데이터에 대한 정답을 주지 않고 학습시키는 방법으로, 클러스터링이나 오토인코더에 주로 쓰인다. **강화학습(reinforcement learning)** : Agent가 주어진 state에 대해 어떤 action을 취하고, 이로부터 reward를 얻으면서 학습을 진행하는 방식이다. 에이전트가 보상을 최대화하는 방식으로 학습이 진행된다. |
| 머신러닝 일반 | ​머신러닝의 단계 | 데이터 수집 → 데이터 전처리 → 모델 선택과 훈련 → 모델 평가와 튜닝 |
| 머신러닝 일반 | ​퍼셉트론 | **Perceptron** : 다수의 입력 신호를 받아 그 신호들의 총합이 임계값을 넘었을때 1을 출력하고, 넘지 못했을때 0, 혹은 -1을 반환하는 알고리즘이다. 단층 퍼셉트론은 XOR 문제를 풀지 못하지만 다층 퍼셉트론은 풀 수 있다. |
| 머신러닝 일반 | ​앙상블 | **Ensemble** :여러 개의 모델을 학습시켜, 그 모델들의 예측 결과를 활용하여 더 정확한 예측값을 구하는 모델이다. 앙상블 학습에는 Bagging(나온 결과를 투표 혹은 평균활용)과 Boosting(가중치 활용) 방식이 있다. |
| 머신러닝 분류 모델 | 선형회귀 | ​Logistic Regression(로지스틱 회귀) : 종속 변수 y와 한 개 이상의 독립변수 X와의 선형 상관 관계를 모델링 하는 기법으로 1번,2번,3번 이라고 바로 분류를 하는 게 아니라 1번과 1번이 아닌것, 2번과 2번이 아닌 것 등으로 분류하여 가장 높은 확률을 가지는 카테고리를 찾아 낸다.  |
| 머신러닝 분류 모델 | ​서포트 벡터 머신 | Support Vector Machine : 데이터를 사상된 공간에 표현했을 때, 데이터의 경계 사이에 선을 그어 최대 마진을 가지는 경계선을 찾는 알고리즘 이다. |
| 머신러닝 분류 모델 | 나이브 베이즈 | Naive Bayse : 각각의 특성들이 독립적이라고 가정하는 조건부 확률을 활용한 모델이다. |
| 머신러닝 분류 모델 | 의사결정 나무 | Decision Tree 결정 트리 학습 : 상위 노드의 의사결정 규칙에 따라 하위 노드로 데이터를 나누어 나가며 분류나 예측을 수행하는 모델이다. |
| 머신러닝 분류 모델 | 랜덤 포레스트 | Random Forest : 의사 결정 나무의 앙상블 모델이다. |
| 머신러닝 분류 모델 | K-최근접 이웃 | K-Nearest Neighbor Algorithm
새로운 데이터를 예측할 때, 훈련 데이터 셋에서 가장 가까운 이웃 데이터를 찾아 예측한다. 여러개의 이웃 데이터를 찾을 경우에는 더 많은 수의 이웃을 따라간다.  |
| 데이터 전처리 | 스케일링 | Data Scailing : 데이터의 특성(feature)끼리 스케일이 다르지 않도록 값을 조정해주는 전처리 과정을, 정규화(Normalization)와 표준화(Standardization)가 있다. |
| 데이터 전처리 | ㄴ정규화 | Nomalization : 각 feature들의 크기를 0과 1의 사이에 맞추는 것을 의미한다. ex. MinMaxScaler가 있다. |
| 데이터 전처리 | ㄴ표준화 | Standardization : 각 feature들의 평균을 0으로, 표준편차를 1로 맞춘다. MinMaxScaler에 비해 이상치에 덜 민감하다. |
| 데이터 전처리 | 규제 | Regularization) : 과적합(Overfitting)을 막기 위해 특정 가중치가 커지지 않도록 제한하는 방식으로 모델 복잡도를 줄인다. L1 규제와 L2 규제가 있다.  |
| 데이터 전처리 | ㄴ L1규제 | cost function 식에서 가충치의 절대값을 더해준다. |
| 데이터 전처리 | ㄴ L2 규제 | 가중치의 제곱값을 이용한다. L1에 비해 이상치(Outlier)나 noise가 있는 데이터에 대한 학습이 좋으며, 특히 선형 모델의 일반화에 좋다. |
| 데이터 전처리 | 과적합을 막기 위한 방법 | 1. 더 많은 훈련 데이터를 모은다. 2.규제를 통해 복잡도를 제한한다. 3.파라미터의 개수가 적은 간단한 모델을 선택한다. 4.데이터 차원을 줄인다. |
| 데이터 압축 | 주성분 분석 | PCA(Principal Component Analysis) : 데이터가 가장 많이 분산된 방향의 basis vector를 찾아내는 과정이다.(비지도 학습 방법을 사용한다.) |
| 데이터 압축 | ㄴPCA의 단계 | 데이터 전처리(표준화) → 공분산 행렬 구성 → 공분산 행령의 고유값, 고유벡터 구하기 → 고유값을 내림차순으로 정렬→ 고유값이 가장 큰 k개 고유 벡터 선택 → 최상위 k개의 고유 벡터로 투영 행렬 W 만듦→ 데이터 셋을 새로운 차원 k로 투영 |
| 데이터 압축 | 선형 판별 분석 | LDA(Linear Discriminant Analysis) : 표본의 k개의 집단을 가장 잘 분리시키는 선(면)에 정사영 시키는 방법이다. (지도학습 방법 사용) |
| 데이터 압축 | ㄴ LDA의 단계 | 데이터 전처리(표준화) → 각 클래스에 대해 d 차원 평균벡터 계산→ 클래스 간의 산포행렬( ${S}_B$)과 클래스 내의 산포행렬(${S}_W$) 구성→ ${S}_B^{-1}{S}_W$ 고유벡터와 고유값 계산→ 고유값을 내림차순으로 정렬→ 고유값이 가장 큰 k개 고유 벡터 선택 →최상위 k개의 고유 벡터로 투영 행렬 W 만듦→ 데이터셋을 새로운 차원 k로 투영 |
| 데이터 압축 | 비선형 매핑 | PCA와 LDA가 모두 선형 변환이라면, 비선형 매핑은 선형적이지 않은 데이터를 분리하기 위한 방법이다. |
| 데이터 압축 | ㄴ 커널 트릭 | Kernel trick : 실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리(더 정확히는 스칼라 곱)을 계산한다. |
| 모델학습 | 활성화 함수 | **Activation function** : 노드로 들어오는 신호에 대해 신호를 전달할 만큼 의미가 있는지 없는지 (가중치가 큰지 아닌지) 판단해주는 함수이다. 대표적으로는 시그모이드 함수, ReLU 함수, tanh 함수 등이 있다. |
| 모델학습 | 비용 함수 | Cost function (=손실 함수, Loss function) : 실제로 구한 값과 정답과의 차이를 연산하는 함수이다. MSE,크로스엔트로피(CrossEnrtopy)등이 있다.  |
| 모델학습 | 오차 역전파 | Error Backpropagation : 나온 결과값을 다시 역으로(Input 방향으로) 오차 정보를 전달하여 오차를 줄이는 방식으로 가중치를 업데이트 하는 방법이다. |
| 모델학습 | 옵티마이저 | Optimaizer : 오차의 최저점을 찾아나갈 때, 학습을 빠르고 안정적으로 해주는 것으로, 학습률이나 gradient등을 다양하게 수정해서 학습시키는 역할을 한다. |
| 모델학습 | ㄴ 경사 하강법 | Gradient Descent : 오차 함수의 최소값을 찾는 방법으로, 미분으로 낮은 쪽의 방향을 더듬더듬 찾아나간다. |
| 모델학습 | 파이프라인 | 데이터가 처리 될 때, 한 처리 단계의 출력이 다음 단계의 입력으로 연결되는 구조를 말한다.  |
| 모델 평가와 선택 | 모델 선택 | 최적의 파라미터(혹은 하이퍼파라미터)를 선택하는 것을 의미한다. |
| 모델 평가와 선택 | ㄴ 홀드아웃 방법 | Holdout method : data set을 train set, test set, eval set으로 분할하여 사용하는 모델 선택 방법이다. train set으로 모델을 훈련하고, eval set은 모델 선택에 사용하며, test set으로 모델 훈련 뒤 성능 평가에 사용된다. |
| 모델 평가와 선택 | ㄴ K-겹 교차검증 | K-Fold Cross Validation : 과적합을 막기위한 모델 선택 방법으로 전체 데이터 셋을 k개로 나누고 한뭉치 씩 돌아가면서 테스트 셋으로 지정하여 모델을 훈련시키는 방식이다. |
| 모델 평가와 선택 | 오차 행렬 | Confusion Matrix : TP(True Positive), TN(True Negative), FP(False Positive), FN(False Negative)이 있다.(여기서 진짜(True)가 붙은게 다 잘 맞춘 것) |
| 모델 평가와 선택 | 성능 평가 지표 | 모델의 성능을 평가할 수 있는 지표로 정확도, 정밀도, 재현율, F1-score 등이 있다.  |
| 모델 평가와 선택 | ㄴ 정확도 | Accuracy : True를 True라고, False를 False라고 옳게 예측한 비율. 즉, 데이터 중에서 모든 ‘True’들의 비율 |
| 모델 평가와 선택 | ㄴ 정밀도 | Precision : 모델이 True라고 분류한 것 중 실제 True인 것의 비율 |
| 모델 평가와 선택 | ㄴ 재현율 | Recall(통계학에서는 sensitivity, 다른 분야에서는 hit rate라고도 한다.) 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율 |
| 모델 평가와 선택 | ㄴ F1-점수 | F1-score : 정밀도와(Precision)와 재현율(recall)의 조화평균으로, label의 수가 불균형적일 때 모델의 성능을 비교적 정확히 평가할 수 있다. |
| 모델 평가와 선택 | 하이퍼파라미터 | 모델의 튜닝 파라미터로, 매개변수라고도 한다. 학습을 통해 직접 학습되는 파라미터가 아닌, 학습 알고리즘 자체의 파라미터 이다. |
| 모델 평가와 선택 | 그리드 서치 | 하이퍼파라미터 값의 최적의 조합을 찾을 수 있는 방법으로, 각 하이퍼파라미터 조합의 모델을 모두 서치에 가장 좋은 점수를 내는 모델을 선택할 수 있도록 도와준다. |
| 라이브러리 | 사이킷런 | Sklearn : 머신러닝에 필요한 모듈들을 모아놓은 라이브러리 |
| 라이브러리 | 텐서플로우 | Tensorflow : 최근 2.0으로 버전을 업그레이드하며 많은 것들이 바뀌었다. 동적 그래프 방식으로 바뀜 |
| 라이브러리 | 파이토치 | Pytorch : 동적 그래프를 구축하여 즉시 실행한다. 엔비디아의 CUDA GPU에 최적화 되어 있다. 파이토치가 후발주자여서 User pool이 텐서플로우보다 적다. |

***

